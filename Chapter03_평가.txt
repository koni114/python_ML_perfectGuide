## Chapter03_ 평가 ##
## ------------------------------------------- ##

분류의 평가 지표
- 정확도(accuracy)
- 오차행렬(Confusion Matrix)
- 정밀도(Precision)
- 재현율(Recall)
- F1 Score
- ROC CURVE

## ------------------------------------------- ##

** 01. 정확도(accuracy)

정확도(accuracy) = 예측 결과가 동일한 데이터 건수 // 전체 예측 데이터 건수
	          =  (TN + TP) / (TN + FP + FN + TP)
* 정확도는 imblanced 한 데이터에서는 적합한 지표가 될 수 없음.

## ------------------------------------------- ##

** 02. 오차 행렬(confusion Matrix)
4분면 행렬에서 실제 레이블 클래스 값과 예측 레이블 클래스 값이 일치하는지를 확인해줌.

	N  	F
N	TN	FP
F	FN	FP

True Negative  : Negative 값 0을 0으로 잘 예측한 개수
False Negative : Neagative 값 0을 1로 잘못 예측한 개수
True Positive   : Positive 값 1을 1로 잘 예측
False Positive  : Positive 값 1을 0으로 잘못 예측

** True/False : 잘 맞춤, 잘못 맞춤
** Negative/Positive : 음성/양성

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, pred)

[Output] array([[405, 0], [45, 0]]) ..

불균형한 이진 분류 데이터 세트에서는 Positive 데이터 건수가 매우 작기 때문에
데이터에 기반한 ML 알고리즘은 Positive 보다는 Negative로 예측 정확도가 높아지는 경향이 발생.

## ------------------------------------------- ##

** 03. 정밀도와 재현율
정밀도와 재현율은 Positive 값에 좀 더 초점을 맞춘 평가 지표
다음과 같이 계산
정밀도(Precision) : TP / (FP + TP)
재현율(recall)      : TP / (FN + TP)

정밀도 : 예측을 Positive로 한 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율
           Positive 예측 성능을 더욱 정밀하게 측정하기 위한 평가 지표로 양성 예측도

재현율 : 실제 값이 Positive인 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율
            민감도 또는 TPR 이라고 부름

False Positive vs False Negative
N -> P                 //          P -> N

재현율이 중요한 지표인 경우는 실제 Positive 양성 데이터를 Negative로 잘못 판단하게 되면
업무상 큰 영향이 발생하는 경우 
ex) 암 발생 판단 여부, 보험 사기, 금융 사기 모델

정밀도가 더 중요한 지표는 실제 Negative 음성 데이터를 Positive로 잘못 판단하게 되는 경우
ex) 스팸 메일. -> 정상 메일이 스팸 메일로 인식되어 아예 오지 않는 경우가 risk 더 큼

결과적으로 두 수치가 극단적으로 차이가 나는 경우는 바람직하지 않음

사이킷런에서 정밀도 계산을 위해 precision_score(), 재현율 계산을 위해 recall_score() API 제공

** 정밀도/재현율 트레이드 오프(Trade-off)
정밀도 또는 재현율이 특별히 강조되어야 할 경우 분류의 결정 임계값을 조정해 
정밀도 또는 재현율의 수치를 높일 수 있음

하지만, 정밀도를 높이면 재현율의 수치가 낮아지는 trade off 관계를 가짐
사이킷런의 분류 알고리즘은 예측 데이터가 특정 레이블에 속하는지를 계산하기 위해 "결정 확률"을 구함

* predict_proba
사이킷런은 개별 데이터별로 예측 확률을 반환하는 predict_proba() 를 제공
predict_proba() 는 학습이 완료된 classifier 객체에서 반환이 가능
테스트 피처 데이터를 입력해주면, 이에 댛나 개별 클래스 예측 확률을 반환

이진 분류에서 predict_proba 함수에서 반환되는 값의
첫번째 컬럼 값은 0 클래스의 예측 확률, 두번째 컬럼 값은 1 클래스의 예측 확률임.

--> 정말 결과가 어떻게 나오는지 확률을 보고 싶다.. 고 하면 predict_proba 함수 이용 !

* Binarizer class
해당 predict_proba 메서드가 반환하는 확률 값에서 임계값을 만족하는 ndarray의 칼럼 위치를 최종
예측 클래스로 결정해주는 class

Binarizer class의 fit_transform() 메서드를 이용해 넘파이 ndarray를 입력하면 입력된 nparray 의
값을 지정된 threshold 보다 같거나 작으면 0, 크면 1를 return

* 임계값 조절하여 정확도, 재현율 조절하기
임계값을 작게 조절하면, 0.5 -> 0.3
더 많은 최종 분류 값이 1로 예측 됨(positive 커짐) -> 재현율은 올라가고, 정확도는 떨어짐

임계값을 크게 조절하면, 0.5 -> 0.6 
더 많은 최종 분류 값이 0으로 예측 됨(positive 작아짐) -> 재현율은 작아지고, 정확도는 올라감

* precision_recall_curve()
인자 -> 실제 값 데이터 세트 / 레이블 값이 1일 때의 예측 확률 값을 입력
레이블 값이 1일 때의 예측 확률 값은 predict_proba(X_test)[:, 1]

일반적으로 해당 함수는 0.11 ~ 0.95정도의 임곗값을 담은 넘파이 ndarray와 이 임곗값에 해당하는
정밀도 및 재현율 값을 담은 넘파이 ndarray 를 반환

반환되는 임곗값이 너무 많으므로, 15단계로 추출해 그때의 정밀도와 재현율 값을 같이 살펴보자

""" example """
# 샘플로 10건만 추출, 임곗값을 15 단계로 추출해 좀 더 큰 값이 임곗값과 그 때의 정밀도와 재현율 값을 같이 살펴보기
from sklearn.metrics import precision_recall_curve

# 레이블 값이 1일 때의 예측 확률을 추출
pred_proba_class1 = lr_clf.predict_proba(X_test)[:, 1]

# 실제 값 데이터 세트와 레이블 값이 1일 때의 예측 확률을 precision_recall_curve 인자로 입력
precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_class1)
print('반환된 분류 결정 임곗값 배열 shape :', thresholds.shape)

# 반환된 입계값 배열 로우가 147건이므로 샘플로 10건만 추출하되, 임곗값을 15 Step(간격)으로 추출
the_index = np.arange(1, thresholds.shape[0], 15)
print('샘플 추출을 위한 임곗값 배열 :', the_index)
print('샘플용 10개의 임곗값 :', thresholds[the_index])

# 15 step 단위로 추출된 임곗값에 따른 정밀도와 재현율 값
print('샘플 임곗값별 정밀도 :', np.round(precisions[the_index], 3))
print('샘플 임곗값별 재현율 :', np.round(recalls[the_index], 3)) 
""" """
--> 추출된 임곗값 샘플 10개에 해당되는 정밀도 값과 재현율 값을 살펴보면 임곗값이 증가할수록 재현율은 감소하고 정밀도는 높아짐
 
재현율과 정밀도를 시각화하기

# 해당 정밀도와 재현율을 곡선으로 시각화 하기

""" example """
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
%matplotlib inline

def precision_recall_curve_plot(y_test, pred_proba_c1):
    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_class1)
    
    #  X축을 threshold 값으로, Y축은 정밀도, 재현율 값으로 각각 plot 수행.
    # 정밀도는 점선으로 표시!
    
    plt.figure(figsize = (8, 6))
    threshold_boundary = thresholds.shape[0]
    
    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle= '--', label = 'precision')
    plt.plot(thresholds, recalls[0:threshold_boundary], label = 'recall')
    
    # threshold 값 X 축의 Scale을 0.1 단위로 변경.
    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start, end, 0.1), 2))
    
    # x축, y축 label과 legend, 그리고 grid 설정
    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')
    plt.legend(); plt.grid()
    plt.show()
    
precision_recall_curve_plot(y_test, pred_proba_class1)
""" """

** 정밀도와 재현율의 맹점

정밀도가 100%가 되는 방법 : 확실한 경우에만 P로 예측 -> FP 가 0이 되므로, 
재현율이 100%가 되는 방법 : 전부 positive로 예측        -> FN 가 0이 됨

즉, 정밀도와 재현율중 한 값만 참조하면 극단적인 수치 조작이 가능
어느 상황에서나 정밀도, 재현율 중 중요한 측정 값이 있을 수 있지만, 두 측정 값 중 한개만 사용하는 것은 위험

## ------------------------------------------- ##

04. F1 스코어
정확도와 재현율을 결합한 지표
정밀도와 재현율이 어느 한 쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가짐

F1 = 2 / (( 1 / recall ) + (1 / precision) )
    = 2 * precision * recall / (precision + recall)

ex) precision : 0.9, recall : 0.1 일 경우, F1 Score : 0.18
    precision  : 0.5, recall : 0.5 일 경우, F1 Score : 0.5

사이킷런은 F1 스코어를 구하기 위해 f1_score() 라는 API 제공

""" example """
from sklearn.metrics import f1_score
f1 = f1_score(y_test, pred)
print('f1 스코어: {0:.4f}'.format(f1))
"""             """

## ------------------------------------------- ##

05. ROC 곡선과 AUC
ROC 곡선과 AUC 스코어는 이진 분류의 예측 성능 측정에서 중요하게 사용되는 지표
ROC 곡선은 FPR(False Positive Rate)이 변할 때 TPR이 어떻게 변하는지를 나타내는 곡선
FPR을 X 축으로, TPR을 Y 축으로 잡으면 FPR의 변화에 따른 TPR의 변화가 곡선 형태로 나타남

TPR은 True Positive Rate의 약자이며, 재현율을 나타냄. ( = 민감도)
TPR에 대응하는 지표로, TNR(True Negative Rate)이라고 불리는 '특이성'이 있음

민감도(TPR)은 실제값   Positive가 정확히 예측돼야 하는 수준을 나타냄
특이성(TNR)은 실제값 Negative가 정확히 예측돼야 하는 수준을 나타냄

TNR은 TN / (FP + TN) .
FPR은 FP  / (FP + TN) 이므로, 1 - TNR 

FPR를 0부터 1까지 어떻게 변경?
임곗값을 변경하면 됨! 분류 결정 임곗값은 Positive 예측 값을 결정하는 확률의 기준이기 때문에 FPR을 0을 만드려면 임곗값을 1로 지정하면 됨
임곗값을 1로 가정하면, Positive 예측 기준이 매우 높기 때문에 분류기가 임곗값보다 높은 확률을 가진 데이터를 Positive로 예측할 수 없기 때문

* FPR을 0으로 만드는 방법 -> 임곗값을 1로 setting
FPR = FP / (FP + TN) 이므로, FP = 0, FPR = 0

* FPR을 1으로 만드는 방법 -> 임곗값을 0로 setting
무조건 positive 이므로, TN = 0, FPR = 1

즉, ROC CURVE는 임곗값을 1부터 0까지 거꾸로 변화시키면서 구한 재현율 곡선의 형태와 비슷! ** ( 이것만이라도 기억)

사이킷런은 ROC 곡선을 구하기 위해 roc_curve() API 제공

* roc_curve
"""
from sklearn.metrics import roc_curve
roc_curve(
    y_true  # 실제 Class 값.
    y_score # predict_prob의 반환 값 array에서 Positive 컬럼의 예측 확률
)
"""
output
- fpr : FPR
- tpr : TPR
- thresholds : 임곗값

## LogisticRegression 객체의 predict-proba() 결과를 다시 이용해, curve() 결과 도출.
## ROC 곡선 시각화

## LogisticRegression 객체의 predict-proba() 결과를 다시 이용해, curve() 결과 도출.
## ROC 곡선 시각화

"""
from sklearn.metrics import roc_curve
pred_proba_class1 =lr_clf.predict_proba(X_test)[:, 1]

fprs, tprs, thresholds = roc_curve(y_test, pred_proba_class1)

def roc_curve_plot(y_test, pred_proba_c1):
    fprs, tprs, thresholds = roc_curve(y_test, pred_proba_c1)
    
    # ROC 곡선을 그래프 곡선으로 그림
    plt.plot(fprs, tprs, label = 'Roc Curves')
    
    # 가운데 직선을 그림
    plt.plot([0, 1], [0, 1],'k--', label = 'Random')
    
    # FPR X 축의 Scale을 0, 1 단위로 변경. X, Y축 명 설정 등
    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start, end, 0.1), 2))
    plt.xlim(0, 1); plt.ylim(0, 1)
    plt.xlabel('FPR(1 - Sensitivity)'); plt.ylabel('TPR( Recall )')
    plt.legend(
)
   
roc_curve_plot(y_test, pred_proba_class1)
"""

* AUC(Area Under Curve)
ROC 곡선 자체는 FPR, TPR의 변화 값을 보는데 사용하며, 실제로 성능 지표로 사용되는 것은
AUC(Area under curve) 값
-> 1에 가까울수록 좋은 값. why? 어떤 임곗값이라도 재현율이 높다는 의미이므로
    완전 random이면 0.5

사이킷런은 AUC를 구하기 위해 roc_auc_score() API를 제공

from sklrean.metrics import roc_auc_curve
pred = lr_clf.predict(X_test)
roc_score = roc_auc_curve(y_test, pred)
print('ROC AUC 값 : {0:.4f}'.format(roc_score))

## ------------------------------------------- ##
피마 인디언 당뇨병 예측
- 로지스틱 회귀는 기본적으로 스케일링을 적용하는 것이 좋음
















