## Chapter03_ 평가 ##
## ------------------------------------------- ##

분류의 평가 지표
- 정확도(accuracy)
- 오차행렬(Confusion Matrix)
- 정밀도(Precision)
- 재현율(Recall)
- F1 Score
- ROC CURVE

** 01. 정확도(accuracy)

정확도(accuracy) = 예측 결과가 동일한 데이터 건수 // 전체 예측 데이터 건수
	          =  (TN + TP) / (TN + FP + FN + TP)
* 정확도는 imblanced 한 데이터에서는 적합한 지표가 될 수 없음.


** 02. 오차 행렬(confusion Matrix)
4분면 행렬에서 실제 레이블 클래스 값과 예측 레이블 클래스 값이 일치하는지를 확인해줌.

	N  	F
N	TN	FP
F	FN	FP

True Negative  : Negative 값 0을 0으로 잘 예측한 개수
False Negative : Neagative 값 0을 1로 잘못 예측한 개수
True Positive   : Positive 값 1을 1로 잘 예측
False Positive  : Positive 값 1을 0으로 잘못 예측

** True/False : 잘 맞춤, 잘못 맞춤
** Negative/Positive : 음성/양성

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, pred)

[Output] array([[405, 0], [45, 0]]) ..

불균형한 이진 분류 데이터 세트에서는 Positive 데이터 건수가 매우 작기 때문에
데이터에 기반한 ML 알고리즘은 Positive 보다는 Negative로 예측 정확도가 높아지는 경향이 발생.

** 03. 정밀도와 재현율
정밀도와 재현율은 Positive 값에 좀 더 초점을 맞춘 평가 지표
다음과 같이 계산
정밀도(Precision) : TP / (FP + TP)
재현율(recall)      : TP / (FN + TP)

정밀도 : 예측을 Positive로 한 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율
           Positive 예측 성능을 더욱 정밀하게 측정하기 위한 평가 지표로 양성 예측도

재현율 : 실제 값이 Positive인 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율
            민감도 또는 TPR 이라고 부름

False Positive vs False Negative
N -> P                 //          P -> N

재현율이 중요한 지표인 경우는 실제 Positive 양성 데이터를 Negative로 잘못 판단하게 되면
업무상 큰 영향이 발생하는 경우 
ex) 암 발생 판단 여부, 보험 사기, 금융 사기 모델

정밀도가 더 중요한 지표는 실제 Negative 음성 데이터를 Positive로 잘못 판단하게 되는 경우
ex) 스팸 메일. -> 정상 메일이 스팸 메일로 인식되어 아예 오지 않는 경우가 risk 더 큼

결과적으로 두 수치가 극단적으로 차이가 나는 경우는 바람직하지 않음

사이킷런에서 정밀도 계산을 위해 precision_score(), 재현율 계산을 위해 recall_score() API 제공

** 정밀도/재현율 트레이드 오프(Trade-off)
정밀도 또는 재현율이 특별히 강조되어야 할 경우 분류의 결정 임계값을 조정해 
정밀도 또는 재현율의 수치를 높일 수 있음

하지만, 정밀도를 높이면 재현율의 수치가 낮아지는 trade off 관계를 가짐
사이킷런의 분류 알고리즘은 예측 데이터가 특정 레이블에 속하는지를 계산하기 위해 "결정 확률"을 구함

* predict_proba
사이킷런은 개별 데이터별로 예측 확률을 반환하는 predict_proba() 를 제공
predict_proba() 는 학습이 완료된 classifier 객체에서 반환이 가능
테스트 피처 데이터를 입력해주면, 이에 댛나 개별 클래스 예측 확률을 반환

이진 분류에서 predict_proba 함수에서 반환되는 값의
첫번째 컬럼 값은 0 클래스의 예측 확률, 두번째 컬럼 값은 1 클래스의 예측 확률임.

--> 정말 결과가 어떻게 나오는지 확률을 보고 싶다.. 고 하면 predict_proba 함수 이용 !

* Binarizer class
해당 predict_proba 메서드가 반환하는 확률 값에서 임계값을 만족하는 ndarray의 칼럼 위치를 최종
예측 클래스로 결정해주는 class

Binarizer class의 fit_transform() 메서드를 이용해 넘파이 ndarray를 입력하면 입력된 nparray 의
값을 지정된 threshold 보다 같거나 작으면 0, 크면 1를 return

* 임계값 조절하여 정확도, 재현율 조절하기
임계값을 작게 조절하면, 0.5 -> 0.3
더 많은 최종 분류 값이 1로 예측 됨(positive 커짐) -> 재현율은 올라가고, 정확도는 떨어짐

임계값을 크게 조절하면, 0.5 -> 0.6 
더 많은 최종 분류 값이 0으로 예측 됨(positive 작아짐) -> 재현율은 작아지고, 정확도는 올라감

* precision_recall_curve()
인자 -> 실제 값 데이터 세트 / 레이블 값이 1일 때의 예측 확률 값을 입력
레이블 값이 1일 때의 예측 확률 값은 predict_proba(X_test)[:, 1]

일반적으로 해당 함수는 0.11 ~ 0.95정도의 임곗값을 담은 넘파이 ndarray와 이 임곗값에 해당하는
정밀도 및 재현율 값을 담은 넘파이 ndarray 를 반환

반환되는 임곗값이 너무 많으므로, 15단계로 추출해 그때의 정밀도와 재현율 값을 같이 살펴보자







