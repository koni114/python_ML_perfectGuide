## Chapter04_분류 ##
## -------------------------------------------------------------- ##
01. 개요
분류는 다양한 머신러닝 알고리즘으로 구현될 수 있음
- 베이즈 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)
- 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)
- 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)
- 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(SVM)
- 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘
- 심층 연결 기반의 신경망(Nerual Network)
- 서로 다른 머신러닝 알고리즘을 결합한 앙상블(Ensemble)

* 앙상블
정형 데이터 같은 경우에 예측 분석에서는 앙상블이 매우 높은 예측 성능으로 인해 
많은 분석가와 데이터 과학자들에게 애용되고 있음.

Bagging과 Boosting 영역으로 나뉨
Bagging의 대표적인 알고리즘인 랜덤 포레스트는 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 
유연성 등으로 많은 분석가가 애용하는 알고리즘.
하지만, 근래의 앙상블 방법은 Boosting 방법으로 지속해서 발전하고 있음.( 진짜?)

부스팅 계열의 효시라고 할 수 있는 GradientBoosting 알고리즘은 뛰어난 예측 성능을 가지고 있지만
너무 오래걸리는 단점이 있어 최적화 모델 튜닝이 어려웠음.
하지만 최근 XgBoost, LightGBM 등 기존 그래디언트 예측 성능을 한 단계 발전시키면서도
수행 시간을 단축시킨 알고리즘이 계속 등장하면서 정형 데이터의 분류 영역에서 활용도가 높은 알고리즘으로
자리 잡음.

* 결정 트리
데이터의 Scaling이나 정규화 등의 사전 가공의 영향이 매우 적음.

예측 성능을 향상시키기 위해 복잡한 규칙 구조를 가져가야 하며,  이로 인한 과적합이 발생해 반대로 예측 성능이 저하될 수도 있다는 단점 존재. 
하지만 이러한 단점이 앙상블 기법에서는 장점이 될 수 있음.

일반적으로 앙상블은 약한 학습기 여러개를 조합해 확률전 보완과 오류가 발생한 부분에 대한 가중치를 계속
업데이트하면서 예측 성능을 향상시키는데, 결정 트리가 좋은 약한 학습기가 되기 때문

## -------------------------------------------------------------- ##

02. 결정 트리
Decision Tree는 ML 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘
데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 Tree 기반의 분류 규칙을 만드는 것.
if. else를 자동으로 찾아내 예측을 위한 규칙을 만들어 줌.

데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 좌우

규칙노드 : 규칙 조건이 있는 Node
결정노드 : 결정된 클래스 값

새로운 규칙 조건마다 서브 트리가 생성됨
많은 규칙이 있다는 것은 방식이 복잡해진다는 얘기고, 이는 곧 과적합으로 이어지기 쉬움. 
즉 트리의 depth가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높음

가능한 한 적은 결정노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록
결정 노드의 규칙이 정해져야 함. 
-> 최대한 균일한 데이터 세트를 구성할 수 있도록 분할하는 것이 필요
	** 균일한 데이터 : 특정 분류 값이 상대적으로 많은 데이터 셋. 

정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 정보 이득(Information Gain)지수와 지니 계수가 있음

* 정보 이득(Information Gain)
엔트로피라는 개념을 기반으로 함. 
엔트로피는 주어진 데이터의 혼잡도를 의미하는데, 서로 다른 값이 많이 섞여있으면 엔트로피가 높고, 같은 값이 섞여있으면 낮음
정보 이득 지수 = (1 - 엔트로피지수 값)
결정 트리는 정보 이득이 높은 속성을 기준으로 분할.

* 지니 계수
원래 경제학에서 불평등 지수를 나타낼 때 사용하는 계수
0이 가장 평등하고 1로 갈수록 불평등
ML에 적용할 때는 의미론적으로 해석되어 데이터가 다양한 값을 나타낼수록 평등하며, 특정 값으로 치우쳤을 경우는 불평등.

사이킷런에서 구현한 DecisionTreeClassifier는 기본으로 * 지니 계수를 이용해 데이터 세트를 이용
결정 트리의 일반적인 알고리즘은 데이터 세트를 분할하는데 가장 좋은 조건(정보 이득, 지니 계수가 높은 조건)을 찾아 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤 
데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정.

** 결정 트리 모델의 특징
'균일도'라는 룰을 기반으로 하고 있어 알고리즘이 쉽고 직관적임
룰이 매우 명확하고 이에 기반해 어떻게 규칙 노드와 리프 노드가 만들어지는지 알 수 있고, 시각화로 표현까지 할 수 있음.
정보의 균일도만 신경쓰면 되므로, 특별한 경우를 제외하곤 피처의 스케일링과 정규화 같은 전처리 작업이 필요 없음

가장 큰 단점은 과적합으로 인한 정확도가 떨어진다는 점
서브 트리를 계속 만들다 보면 피처가 많고 균일도가 다양하게 존재할수록 트리의 깊이가 커지고 복잡해질 수 밖에 없음
-> 트리 크기를 사전에 제한하는 것이 오히려 성능 튜닝에 도움이 됨

* 결정 트리 파라미터
DecisionTreeClassifier, DecisionTreeRegressor 클래스 제공
DecisionTreeClassifier  : 분류를 위한 클래스
DecisionTreeRegressor : 회귀를 위한 클래스
두 클래스는 동일한 파라미터 사용

- min_samples_split : 노드를 분할하기 위한 최소한의 샘플 데이터 수.
		   과적합을 제어하는 데 사용
		   default : 2, 작게 설정할수록 노드가 많아져서 과적합 가능성 증가
		   과적합을 제어. 1로 설정할 경우 노드가 많아져서 과적합 증가

- min_samples_leaf : 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수
		  Min_samples_split과 유사하게 과적합 제어 용도. 비대칭적 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로,
	  	  이 경우는 작게 설정 필요

- max_features      : 최적의 분할을 위해 고려할 최대 피처 개수. 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행
		 int형으로 지정하면 대상 피처의 개수, float 형으로 지정하면 전체 피처 중 대상 피처의 퍼센트임
		 'sqrt' 는 전체 피처 중 sqrt(전체피처개수)
	 	 'auto'로 지정하면 sqrt와 동일
		 'log'는 전체 피처 중 log2(전체피처개수) 선정
		 'None'은 전체 피처 선정
- max_depth         : 트리의 최대 깊이 규정
		 디폴트는 None, None으로 설정하면 완벽하게 클래스 결정 값이 될 때 까지 깊이를 계속 키우며 분할하거나 노드가 가지는 데이터 개수가 min_samples_split보다 작아질 때까지
		 계속 깊이를 증가 시킴
		 깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합할 수 있으므로 적절한 값으로 제어 필요

- max_leaf_nodes   : 말단 노드의 최대 개수

** 결정 트리 모델 시각화
Graphviz 패키지를 사용 : 그래프 기반의 dot 파일로 기술된 다양한 이미지를 쉽게 시각화 할 수 있는 패키지
사이킷런은 Graphviz 패키지와 쉽게 인터페이스 할 수 있도록 export_graphaiz() API 제공
함수 인자로 1. 학습이 완료된 Estimator, 2. 피처의 이름 리스트, 3. 레이블 이름 리스트를 입력하면 학습된 결정 트리 규칙을 실제 트리 형태로 시각화해 보여줌.

Graphviz 는 파이썬으로 개발된 패키지가 아닌, C++로 운영 체제에 포팅된 패키지 이므로, 이를 파이썬 기반의 모듈과 인터페이스해주기 위해서는 Graphviz를 설치한 뒤
파이썬과 인터페이스할 수 있는 파이썬 래퍼(Wrapper) 모듈을 별도로 설치



